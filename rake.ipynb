{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string \n",
    "import unicodedata as ud\n",
    "from greek_stemmer import GreekStemmer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import indexer as ind\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = list(database.find({\"_id\":\"1\"}, { \"_id\": 0, \"speech\": 1 }))\n",
    "speech = document[0]['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Παρακαλείται ο κύριος Γραμματέας να συνοδεύσει την Ιερά Σύνοδο εκτός της Αιθούσης της Βουλής.\n",
      " \n",
      ".\n",
      "Παρακαλείται ο συνάδελφος Βουλευτής κ. Σαδίκ Αμέτ, που ανήκει στο Μωαμεθανικό Θρήσκευμα να προσέλθει και να δώσει τον οριζόμενο από το Σύνταγμα όρκο επί του Κορανίου.\n",
      " :\n",
      "~\"Ορκίζομαι στο όνομα του Παντοδύναμου Θεού και του μόνου αυτού Προφήτη ο οποίος είναι ο Μωάμεθ να είμαι πιστός στην πατρίδα και το δημοκρατικό πολίτευμα, να υπακούω στο Σύνταγμα και τους νόμους και να εκπληρώνω ευσυνείδητα τα καθήκοντά μου\".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('el_core_news_md')\n",
    "doc = nlp(speech)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# choose the segmentation function you need/prefer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents:\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m segmenter\u001b[39m.\u001b[39mprocess(sentence):\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(txt)\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\syntok\\segmenter.py:45\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(document, bracket_skip_len)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mSegment a document into paragraphs, sentences, and tokens.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m:return: an iterator over paragraphs and sentences as lists of tokens\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m tok \u001b[39m=\u001b[39m Tokenizer()\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfor\u001b[39;00m paragraph \u001b[39min\u001b[39;00m preprocess(document):\n\u001b[0;32m     46\u001b[0m     \u001b[39myield\u001b[39;00m segment(tok\u001b[39m.\u001b[39mtokenize(paragraph), bracket_skip_len)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\syntok\\segmenter.py:58\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m     50\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    Split text bodies into paragraphs and\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m    join hyphenated words across line-breaks.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m    :return: a list of paragraphs\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m __PARAGRAPH_SEP\u001b[39m.\u001b[39msplit(\n\u001b[1;32m---> 58\u001b[0m         Tokenizer\u001b[39m.\u001b[39;49mjoin_hyphenated_words_across_linebreaks(text)\n\u001b[0;32m     59\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\syntok\\tokenizer.py:104\u001b[0m, in \u001b[0;36mTokenizer.join_hyphenated_words_across_linebreaks\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin_hyphenated_words_across_linebreaks\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    103\u001b[0m     \u001b[39m\"\"\"Join 'hyhen-\\\\n ated wor- \\\\nds' to 'hyphenated words'.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m Tokenizer\u001b[39m.\u001b[39;49m_hyphen_newline\u001b[39m.\u001b[39;49msubn(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, text)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "import syntok.segmenter as segmenter\n",
    "\n",
    "# choose the segmentation function you need/prefer\n",
    "for sentence in doc.sents:\n",
    "    for txt in segmenter.process(sentence):\n",
    "        print(txt)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(text, w, n, theta, language):\n",
    "    #STEP 1: SPLIT TEXT INTO SENTENCES AND PRE-PROCESS \n",
    "    nlp = spacy.load('el_core_news_md')\n",
    "    sentences = nlp(speech)\n",
    "    for sentence in sentences.sents:\n",
    "        #SPLIT SENTENCE INTO CHUNKS\n",
    "        import segtok.segmenter as sn\n",
    "        chunks = sn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
