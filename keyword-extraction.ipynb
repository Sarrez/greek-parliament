{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string \n",
    "import unicodedata as ud\n",
    "from greek_stemmer import GreekStemmer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import indexer as ind\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidate domain-specific stopwords and clean index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdocs = list(index.find({ }, { \"_id\": 0, \"list.numdoc\": 1 }))\n",
    "x = [numdocs[i]['list']['numdoc'] for i in range(len(numdocs))]\n",
    "numdocs = {}\n",
    "for numdoc in set(x):\n",
    "    numdocs[numdoc] = x.count(numdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Υπαρχουν 1 documents με numdoc==1267\n"
     ]
    }
   ],
   "source": [
    "print(f'Υπαρχουν {numdocs[1267]} documents με numdoc==1267')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3052.,  794.,  315.,  180.,  110.,   86.,   63.,   43.,   38.,\n",
       "          22.,   20.,   14.,   14.,   12.,   11.,    8.,    7.,    4.,\n",
       "           8.,    5.,    4.,    5.,    5.]),\n",
       " array([     0.,   5000.,  10000.,  15000.,  20000.,  25000.,  30000.,\n",
       "         35000.,  40000.,  45000.,  50000.,  55000.,  60000.,  65000.,\n",
       "         70000.,  75000.,  80000.,  85000.,  90000.,  95000., 100000.,\n",
       "        105000., 110000., 115000.]),\n",
       " <BarContainer object of 23 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n_bins = 20\n",
    "\n",
    "# Generate two normal distributions\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "axs.hist(numdocs, bins=[i for i in range(0,120000,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = list(index.find({\"list.numdoc\":{\"$gt\":100000}}, { \"_id\": 1, \"list.numdoc\": 1 }))\n",
    "print([threshold[i]['_id'] for i in range(len(threshold))])\n",
    "with open('domain-specific-stopwords.txt', 'w', encoding='utf8') as f:\n",
    "    for i in range(len(threshold)):\n",
    "        f.write(threshold[i]['_id'])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0014', 'στουριδ', '0016', '0017', '0018', '0025', '0026', 'μικροϋπολογισμο', 'τεκτεν', 'στρογγυλευμεν', 'απαγορευοντ', 'αυξηντ', 'εθνοβορ', 'ενεργηθεντ', 'κουτσογιωρ', '1584/23', '1584/87', '671/1982', '2727/75', 'm/v', 'κουραζομεθ', '0062', 'διετρ', 'bενιζελου', '0067', 'αλληλοπαραχωρης', '0070', '0071', 'κοκιν', 'λαϊκιστικοι', 'κατακρεουγ', 'τρομοκρατουντ', 'λουδοβικ', 'παινεμ', 'σεσκουλ', 'γιασπερς', 'ακυριολεξ', 'επραττατ', 'τιραντ', 'σιχας', 'αντεκδικειτα', 'επανελαμβαν', 'χολοσκατ', 'ρεβεραντζ', 'παρεπεμποντ', 'ταυτοσιμ', 'ζγουριδ', 'δημοσθ', 'επικυρουτα', 'επιδειχθεντ', '0112', 'χασομ', 'κρινομ', '68/88', '0117', '0118', 'προσορμιστ', 'φλαμινγκ', 'ξαναφερθ', 'πελωψ', 'μαστευ', 'προδικασατ', 'συνεκαλυψ', 'παρεμποδισατ', '0131', 'αποσχομ', 'εφτακαθαρ', 'σχηματιζατ', 'σταλικ', 'εγεννωντ', 'ραϊμενταλ', 'αιθουσασελιδ', '0159', 'ζητωκραυγαστ', 'εξαιρετον', '0164', 'μαβερικ', '0166', 'συνεδριασις', 'linder', 'σημενς', '0170', 'εκτυπουτα', 'εξεδιωχθην', 'βερει', 'δικηγορισκ', 'πραγματισελιδ', '0190', 'κατωκ', '0192', '0195', 'γρονθοκοπουμεν', 'ειπωθεν', 'ξανανταμως', '0201', 'δαμοφλ', 'σμυκρινς', 'λεβεντογιανν', '0210', 'ενομισθ']\n"
     ]
    }
   ],
   "source": [
    "threshold = list(index.find({\"list.numdoc\":2}, { \"_id\": 1, \"list.numdoc\": 1 }))\n",
    "print([threshold[i]['_id'] for i in range(len(threshold))][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': '000/8/245258/σ'}, {'_id': '000/οικ'}, {'_id': '0000%απο'}, {'_id': '00010ο'}, {'_id': '00012%απο'}, {'_id': '00016%απο'}, {'_id': '0002%απο'}, {'_id': '00020%απο'}, {'_id': '00024%απο'}, {'_id': '0002γραφημα'}, {'_id': '0002θεωρουμε'}, {'_id': '0003%απο'}, {'_id': '0003επιλεξει'}, {'_id': '0004γων'}, {'_id': '0006%απο'}, {'_id': '0006που'}, {'_id': '0008%απο'}, {'_id': '0008μοκρατια'}, {'_id': '0008ο'}, {'_id': '0009ορισμενες'}, {'_id': '000αλλοτε'}, {'_id': '000αυτα'}, {'_id': '000β'}, {'_id': '000γ'}, {'_id': '000για'}, {'_id': '000δ'}, {'_id': '000διδακτικο'}, {'_id': '000δραχμες'}, {'_id': '000δραχμων'}, {'_id': '000δρχ'}, {'_id': '000ε'}, {'_id': '000επι'}, {'_id': '000ετος'}, {'_id': '000ευρω'}, {'_id': '000η'}, {'_id': '000και'}, {'_id': '000καταστηματαρχες'}, {'_id': '000μοναδων'}, {'_id': '000νται'}, {'_id': '000ο'}, {'_id': '000πανω'}, {'_id': '000προηγουμενησμετ'}, {'_id': '000σελιδα'}, {'_id': '000στρ'}, {'_id': '000στρεμματα'}, {'_id': '000στρεμματων'}, {'_id': '000συνολο'}, {'_id': '000τ'}, {'_id': '000τα'}, {'_id': '000υπερβαλλον'}, {'_id': '000χιλ/νων'}, {'_id': '000ως'}, {'_id': '000–γιατι'}, {'_id': '000–και'}, {'_id': '000–συν'}, {'_id': '000–τοσα'}, {'_id': '0010λογη'}, {'_id': '0010μερη'}, {'_id': '0010νομους'}, {'_id': '0011δυο'}, {'_id': '0012/πολ'}, {'_id': '0012αναφερθω'}, {'_id': '0012βητηση'}, {'_id': '0013απο'}, {'_id': '0014περιοριζετε'}, {'_id': '0016των'}, {'_id': '0017αρθρο'}, {'_id': '0018ξεφορτωνονται'}, {'_id': '0019τονους'}, {'_id': '0021γιατι'}, {'_id': '0021μπορουμε'}, {'_id': '0021χως'}, {'_id': '0022/σχετ'}, {'_id': '0022μικρο'}, {'_id': '0022προϋπολογισμος'}, {'_id': '0023εκπροσωπει'}, {'_id': '0023ρηματικο'}, {'_id': '0023ψεων'}, {'_id': '0025το'}, {'_id': '0025χρονικα'}, {'_id': '0026υπαρχουν'}, {'_id': '0027δου'}, {'_id': '0028δικαιολογημενων'}, {'_id': '0028δωσε'}, {'_id': '0029διατυπωση'}, {'_id': '003/2007]σημειωση'}, {'_id': '0030θεση'}, {'_id': '0030τειας'}, {'_id': '0032εγινε'}, {'_id': '0032τηθει'}, {'_id': '0033την'}, {'_id': '0034γες'}, {'_id': '0034και'}, {'_id': '0034πρωινες'}, {'_id': '0035για'}, {'_id': '0035μησεις'}, {'_id': '0036στα'}, {'_id': '0036συνδεομενο'}, {'_id': '0036ταν'}, {'_id': '0037εκμεταλλευονται'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "to_remove = list(index.find({\"_id\":{\"$regex\":\"^[0-9][0-9][0-9].*[α-ω]$\"}},{ \"_id\": 1, \"list\": 0}))\n",
    "#x = re.findall(\"^[0-9][0-9][0-9].*[α-ω]$\", txt)\n",
    "print(to_remove[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Document Keyword Extraction with python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'LazyCorpusLoader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m document \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(database\u001b[39m.\u001b[39mfind({\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m}, { \u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m }))\n\u001b[0;32m      2\u001b[0m doc \u001b[39m=\u001b[39m document[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m words_in_row \u001b[39m=\u001b[39m ind\u001b[39m.\u001b[39;49mpreprocess_doc(document[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mspeech\u001b[39;49m\u001b[39m'\u001b[39;49m], stopwords)\n",
      "File \u001b[1;32mc:\\Users\\harry\\official-greek-parliament\\greek-parliament\\indexer.py:54\u001b[0m, in \u001b[0;36mpreprocess_doc\u001b[1;34m(doc, stopwords)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m#doc = doc.lower()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m doc\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     words \u001b[39m=\u001b[39m [stemmer\u001b[39m.\u001b[39mstem(w\u001b[39m.\u001b[39mupper())\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, re\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m[,~`; _\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m-!?:]+\u001b[39m\u001b[39m'\u001b[39m,doc)) \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords \u001b[39mand\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation]\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m words\n",
      "File \u001b[1;32mc:\\Users\\harry\\official-greek-parliament\\greek-parliament\\indexer.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m#doc = doc.lower()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m doc\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     words \u001b[39m=\u001b[39m [stemmer\u001b[39m.\u001b[39mstem(w\u001b[39m.\u001b[39mupper())\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, re\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m[,~`; _\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m-!?:]+\u001b[39m\u001b[39m'\u001b[39m,doc)) \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m stopwords \u001b[39mand\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation]\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m words\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'LazyCorpusLoader' is not iterable"
     ]
    }
   ],
   "source": [
    "document = list(database.find({\"_id\":\"1\"}, { \"_id\": 0, \"speech\": 1 }))\n",
    "doc = document[0]['speech']\n",
    "words_in_row = ind.preprocess_doc(document[0]['speech'], stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "with open('stopwords.txt', encoding='utf-8') as file:\n",
    "  stopwords = [line.rstrip() for line in file]\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=stopwords)\n",
    "keywords = kw_extractor.extract_keywords(doc)\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_keywords(database, \"0\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Document Keyword Extraction with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_of_words(words_in_row:list)->nx.DiGraph:\n",
    "    \"\"\"Creates a graph of words for a single document\n",
    "    \n",
    "    \"\"\"\n",
    "    g = nx.DiGraph()\n",
    "    #add unique words as nodes of the graph\n",
    "    g.add_nodes_from(set(words_in_row))\n",
    "    for j,word in enumerate(words_in_row):\n",
    "        #add word as a node if it doesnt exist\n",
    "        #if word not in g.nodes:\n",
    "            #g.add_node(word)    \n",
    "        #generate list of indexes\n",
    "        gen = (x for x in range(j+1,j+3) if x<len(words_in_row))\n",
    "        for k in gen:\n",
    "            #avoid self-loops\n",
    "            if(word == words_in_row[k]):\n",
    "                pass\n",
    "            #if edge does not exist create it, else increase edge weight\n",
    "            elif((word, words_in_row[k]) not in g.edges):\n",
    "                g.add_edge(word, words_in_row[k])\n",
    "                g[word][words_in_row[k]]['weight'] = 1\n",
    "            else:\n",
    "                g[word][words_in_row[k]]['weight'] += 1\n",
    "    return g\n",
    "\n",
    "def generate_graphs(total_documents:int, chunksize:int, database_collection):\n",
    "    \"\"\"Creates graphs of words for documents of a given collection of documents\n",
    "    \n",
    "    \"\"\"\n",
    "    chunk = []\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    #get inverted index\n",
    "    ticks = [x for x in range(0,total_documents,chunksize)]\n",
    "    ticks.append(total_documents)\n",
    "    graphs=[]\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    for j in range(len(ticks)-1):\n",
    "        tokens = {}\n",
    "        chunk = list(database_collection.find({ }, { \"_id\": 1, \"speech\": 1 })[ticks[j]:ticks[j+1]])\n",
    "        print(\"Length of chunk: \", len(chunk))\n",
    "        size_distribution = []\n",
    "        #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "        #for each speech\n",
    "        for i, row in enumerate(chunk):\n",
    "            words_in_row = ind.preprocess_doc(row[\"speech\"], stopwords)\n",
    "            graph = create_graph_of_words(words_in_row)\n",
    "            graphs.append(graph)\n",
    "   \n",
    "        #insert chunk of tokens to a mongo collection named index\n",
    "        \n",
    "        print(\"CHUNK\", counter, \" FINISHED\")\n",
    "    \n",
    "    return graphs\n",
    "def weighted_undirected_k_core(graph:nx.Graph):\n",
    "    k = 0\n",
    "    core_numbers = dict([(node,0) for node in graph.nodes])\n",
    "    degrees = dict(graph.degree(weight='weight'))\n",
    "    while(len(graph.nodes)>0):\n",
    "        min_node = get_min_node(degrees)\n",
    "        #print(min_node)\n",
    "        if(degrees[min_node]>k):\n",
    "            core_numbers[min_node] = degrees[min_node]\n",
    "            k = core_numbers[min_node]\n",
    "        else:\n",
    "            core_numbers[min_node] = k\n",
    "        for neighbor in graph.neighbors(min_node):\n",
    "            degrees[neighbor] = degrees[neighbor]  - graph.get_edge_data(neighbor, min_node)['weight']\n",
    "        graph.remove_node(min_node)\n",
    "        del degrees[min_node]\n",
    "        \n",
    "    return core_numbers\n",
    "\n",
    "def get_min_node(degrees):\n",
    "    #sort degrees in increasing orders\n",
    "    degrees = sorted(degrees.items(), key=lambda d: d[1])\n",
    "    return degrees[0][0]\n",
    "\n",
    "#returns the keywords for a document\n",
    "def get_keywords(database_collection, document_id:string)->list:\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    with open('domain-specific-stopwords.txt', encoding='utf-8') as file:\n",
    "        domain_specific_stopwords = [line.rstrip() for line in file]\n",
    "    tokens = {}\n",
    "    #read documents from MongoDB in chunks\n",
    "    document = list(database_collection.find({\"_id\":document_id}, { \"_id\": 0, \"speech\": 1 }))\n",
    "    size_distribution = []\n",
    "    #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "    #for each speech\n",
    "    words_in_row = ind.preprocess_doc(document[0]['speech'], stopwords)\n",
    "    words_in_row = [word for word in words_in_row if word not in domain_specific_stopwords]\n",
    "    graph = create_graph_of_words(words_in_row)\n",
    "    #extract keywords and insert to mongo\n",
    "    #core_numbers = weighted_undirected_k_core(graph)\n",
    "    centrality = nx.pagerank(graph)\n",
    "    centrality = dict(sorted(centrality.items(), key=lambda item: -item[1]))\n",
    "    #print(centrality)\n",
    "    words = list(centrality.keys())\n",
    "    keywords = words[:int(len(words_in_row)/4)]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "#returns the keywords given a directed, unweighted graph of words\n",
    "def extract_keywords(graph_of_words:nx.DiGraph)->list:\n",
    "    \"\"\"Extracts keywords from a graph of words\n",
    "    \n",
    "    \"\"\"\n",
    "    main_core = nx.k_core(graph_of_words)\n",
    "    print(\"Graph size: \", len(graph_of_words.nodes))\n",
    "    print(\"Main core size: \", len(main_core.nodes))\n",
    "    main_core_centrality = nx.in_degree_centrality(main_core)\n",
    "    print(main_core_centrality)\n",
    "    values = main_core_centrality.values()\n",
    "    keys = main_core_centrality.keys()\n",
    "    \n",
    "    #ToDo: find a criterion on which keywords to keep from main core\n",
    "    import statistics as stats\n",
    "    median = (stats.median(values))\n",
    "    keywords = [keyword for keyword in main_core_centrality.keys() if main_core_centrality[keyword]>=median] \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['προγραμμ', 'εργ', 'ελλαδ', 'αλλαγ', 'εποχ', 'προχωρ', 'αποκρατικοποιης', 'λα', 'ηδη', 'τομ', 'σημαντικ', 'αγαπητ', 'διοικης', 'καλ', 'φορολογ', 'εκσυγχρονισμ', 'εξυγιανς', 'κρατ', '1991', 'βαρ']\n",
      "\n",
      "Keyphrase:  Μέγαρο Βουλής Αγαπητέ : score 0.016875460054915633\n",
      "Keyphrase:  Τσαλδάρη Μέγαρο Βουλής : score 0.018061732484664847\n",
      "Keyphrase:  Αθανάσιο Τσαλδάρη Μέγαρο : score 0.02139547982025511\n",
      "Keyphrase:  κύριοι συνάδελφοι : score 0.02363532523553325\n",
      "Keyphrase:  Παπανδρέου Πρόεδρος Πανελληνίου : score 0.03528529969250768\n",
      "Keyphrase:  Πρόεδρος Πανελληνίου Σοσιαλιστικού : score 0.03528529969250768\n",
      "Keyphrase:  Πανελληνίου Σοσιαλιστικού Κινήματος : score 0.03528529969250768\n",
      "Keyphrase:  Βουλής : score 0.037353559438839376\n",
      "Keyphrase:  Βουλής κύριο Αθανάσιο : score 0.039177003790680065\n",
      "Keyphrase:  κύριο Αθανάσιο Τσαλδάρη : score 0.04677562204752342\n"
     ]
    }
   ],
   "source": [
    "#get keywords for a single document\n",
    "keywords = get_keywords(database, \"63428\")\n",
    "print(keywords[:20])\n",
    "import yake\n",
    "\n",
    "with open('stopwords.txt', encoding='utf-8') as file:\n",
    "  stopwords = [line.rstrip() for line in file]\n",
    "document = list(database.find({\"_id\":\"20\"}, { \"_id\": 0, \"speech\": 1 }))\n",
    "doc = document[0]['speech']\n",
    "words_in_row = ind.preprocess_doc(document[0]['speech'], stopwords)\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=stopwords)\n",
    "keywords = kw_extractor.extract_keywords(doc)\n",
    "print( )\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def transform_uni_to_ngram(unigram_keywords, text):\n",
    "  \"\"\"\n",
    "  Candidate unigrams that form ngrams in the text\n",
    "  are merged and now form a single candidate keyphrase.\n",
    "  \"\"\"\n",
    "  # set the spacy model\n",
    "  nlp = spacy.load(\"el_core_news_md\")\n",
    "\n",
    "  # function that tells if spacy token is keyword or not\n",
    "  ckw_getter = lambda token: token.text.lower() in unigram_keywords\n",
    "  \n",
    "  # set that function as token extension (as attribute with ._)\n",
    "  #token = token.set_extension(\"is_ckw\", getter=ckw_getter, force=True)\n",
    "\n",
    "  # variable to store resulting keyphrases\n",
    "  res = []\n",
    "  \n",
    "  # split original text in sentences\n",
    "  sentences = nltk.sent_tokenize(text)\n",
    "  \n",
    "  for sent in sentences:\n",
    "      # start keyphrase as empty string\n",
    "      merged_tokens = ''\n",
    "      \n",
    "      # process sentence with spacy model defined beforehand\n",
    "      sent = nlp(sent)\n",
    "      \n",
    "      for token in sent:\n",
    "          # if token is candidate KW (according to func defined before)\n",
    "          token = token.set_extension(\"is_ckw\", getter=ckw_getter, force=True)\n",
    "          if token._.is_ckw:\n",
    "              print(token)\n",
    "              # add it to keyphrase\n",
    "              merged_tokens += token.text.lower() + ' '\n",
    "         \n",
    "         # means that sequence of tokens in text that are candidate KW ends\n",
    "          else:\n",
    "              # check if keyphrase not empty and already not in list of results\n",
    "              if merged_tokens != '' and merged_tokens.strip() not in res:\n",
    "                  \n",
    "                  # only add keyphrases for now, no unigrams\n",
    "                  if len(merged_tokens.strip().split(' ')) != 1:\n",
    "                      res.append(merged_tokens.strip())\n",
    "                  \n",
    "                  # set keyphrase as empty string again\n",
    "                  merged_tokens = ''\n",
    "\n",
    "\n",
    "  for uni in unigram_keywords:\n",
    "      # only add unigram KW to results list if not there already\n",
    "      if uni not in res:\n",
    "          \n",
    "          add_uni = True\n",
    "          \n",
    "          for other in res:  \n",
    "              # only add unigram KW to results list if not used in keyphrase somewhere\n",
    "              if uni in other:\n",
    "                  add_uni = False\n",
    "          \n",
    "          if add_uni:\n",
    "              res.append(uni)   \n",
    "  \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[39m=\u001b[39m document \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(database\u001b[39m.\u001b[39mfind({\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m63428\u001b[39m\u001b[39m\"\u001b[39m}, { \u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m }))\n\u001b[1;32m----> 3\u001b[0m aaaa \u001b[39m=\u001b[39m transform_uni_to_ngram(keywords,text[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mspeech\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn [28], line 32\u001b[0m, in \u001b[0;36mtransform_uni_to_ngram\u001b[1;34m(unigram_keywords, text)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sent:\n\u001b[0;32m     30\u001b[0m     \u001b[39m# if token is candidate KW (according to func defined before)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     token \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mset_extension(\u001b[39m\"\u001b[39m\u001b[39mis_ckw\u001b[39m\u001b[39m\"\u001b[39m, getter\u001b[39m=\u001b[39mckw_getter, force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39mis_ckw:\n\u001b[0;32m     33\u001b[0m         \u001b[39mprint\u001b[39m(token)\n\u001b[0;32m     34\u001b[0m         \u001b[39m# add it to keyphrase\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_'"
     ]
    }
   ],
   "source": [
    "text = document = list(database.find({\"_id\":\"63428\"}, { \"_id\": 0, \"speech\": 1 }))\n",
    "\n",
    "aaaa = transform_uni_to_ngram(keywords,text[0]['speech'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, pls ignore\n",
    "dataframe1 = list(database.find({ }, { \"_id\": 1, \"speech\": 1 })[:100000])\n",
    "for i, row in enumerate(dataframe1):\n",
    "    #print(row[\"_id\"])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(graphs[0].edges)\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.savefig('graph.png')\n",
    "# larger figure size\n",
    "#pos = nx.circular_layout(graphs[2])\n",
    "\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw_networkx(graphs[0], with_labels=True,node_size=80,font_size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keywords for a parliament member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distinct parliament member names in a list\n",
    "member_names = database.distinct(\"member_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_member_keywords(member_name:string)->list:\n",
    "    #get ids of member's total speeches\n",
    "    member_speeches = list(database.find({\"member_name\": member_name}, { \"_id\": 1, \"speech\": 0 }))\n",
    "    total_keywords = []\n",
    "    for speech in member_speeches:\n",
    "        keywords = get_keywords(database, speech['_id'])\n",
    "        total_keywords.extend(keywords)\n",
    "    keyword_frequency = {}\n",
    "    for keyword in set(total_keywords):\n",
    "        keyword_frequency[keyword] = total_keywords.count(keyword)\n",
    "    keyword_frequency = sorted(keyword_frequency.items(), key=lambda d: d[1], reverse=True)\n",
    "    return keyword_frequency[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_member_keywords(member_name=member_names[10]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networkx library test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx library\n",
    "import networkx as nx\n",
    "g = nx.Graph()\n",
    "l = ['1','2','3']\n",
    "#g.add_nodes_from(l)\n",
    "\n",
    "for word in l:\n",
    "    if(word not in list(g.nodes)):\n",
    "        g.add_node(word)\n",
    "for e in list(g.nodes):\n",
    "    print(e)\n",
    "    \n",
    "g.add_edge('1','2')\n",
    "g.add_edge('2','3')\n",
    "if(('1','2') in g.edges):\n",
    "    print(' in')\n",
    "for e in list(g.edges):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphs = generate_graphs(100,100,database)\n",
    "graph = graphs[78]\n",
    "for node in graph.nodes:\n",
    "    for neighbor in graph.neighbors(node):\n",
    "        if(graph.get_edge_data(neighbor, node)['weight']>1):\n",
    "            print(graph.get_edge_data(neighbor, node)['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_numbers = dict(core_numbers)\n",
    "max_core = [num for num in core_numbers.values() if(num==2)]\n",
    "print(len(max_core))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nx.Graph()\n",
    "test.add_nodes_from([1,2,3,4,5,6])\n",
    "test.add_edge(1,2)\n",
    "test[1][2]['weight'] = 2\n",
    "test.add_edge(1,4)\n",
    "test[1][4]['weight'] = 4\n",
    "\n",
    "test.add_edge(2,3)\n",
    "test[2][3]['weight'] = 3\n",
    "\n",
    "test.add_edge(3,4)\n",
    "test[3][4]['weight'] = 5\n",
    "\n",
    "test.add_edge(3,5)\n",
    "test[3][5]['weight'] = 7\n",
    "\n",
    "test.add_edge(4,5)\n",
    "test[4][5]['weight'] = 6\n",
    "\n",
    "test.add_edge(5,6)\n",
    "test[5][6]['weight'] = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "core_numbers = dict([(node,0) for node in test.nodes])\n",
    "#print(graph.nodes)\n",
    "print('sdfdsf.',len(nx.k_core(test).nodes))\n",
    "degrees = dict(test.degree(weight='weight'))\n",
    "while(len(test.nodes)>0):\n",
    "    min_node = get_min_node(degrees)\n",
    "    #print(min_node)\n",
    "    if(degrees[min_node]>k):\n",
    "        core_numbers[min_node] = degrees[min_node]\n",
    "        k = core_numbers[min_node]\n",
    "    else:\n",
    "        core_numbers[min_node] = k\n",
    "    for neighbor in test.neighbors(min_node):\n",
    "        print(neighbor)\n",
    "        degrees[neighbor] = degrees[neighbor]  - test.get_edge_data(min_node, neighbor)['weight']\n",
    "    test.remove_node(min_node)\n",
    "    del degrees[min_node]\n",
    "    \n",
    "print(sorted(core_numbers.items(), key=lambda d: d[1]))\n",
    "print(len(core_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graphs[0]\n",
    "#in+out degree\n",
    "def get_graph_degrees(graph):\n",
    "    degrees = [deg for deg in nx.degree(graph)]\n",
    "    degrees = sorted(degrees,key=lambda d: d[1])\n",
    "    degrees = dict(degrees)\n",
    "    return degrees\n",
    "core = {}\n",
    "degrees = get_graph_degrees(graph)\n",
    "print((degrees))\n",
    "for vertex in degrees.keys():\n",
    "    #print(\"Checking vertex: \", vertex)\n",
    "    core[vertex] = degrees[vertex]\n",
    "    neighbors = list(graph.predecessors(vertex)) + list(graph.successors(vertex))\n",
    "    for neighbor in neighbors:\n",
    "        #print(f\"     Checking neighbors of vertex: {vertex}->{neighbor}\")\n",
    "        if(degrees[neighbor]>degrees[vertex]):\n",
    "            degrees[neighbor] = degrees[neighbor] - 1\n",
    "            #reorder degreesdictionary accordingly\n",
    "            degrees = dict(sorted(degrees.items(),key=lambda d: d[1]))\n",
    "print(core)\n",
    "print((nx.k_core(graph).nodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
