{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string \n",
    "import unicodedata as ud\n",
    "from greek_stemmer import GreekStemmer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import indexer as ind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Document Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_of_words(words_in_row:list)->nx.DiGraph:\n",
    "    g = nx.DiGraph()\n",
    "    #add unique words as nodes of the graph\n",
    "    #g.add_nodes_from(set(words_in_row))\n",
    "    for j,word in enumerate(words_in_row):\n",
    "        #add word as a node if it doesnt exist\n",
    "        if word not in g.nodes:\n",
    "            g.add_node(word)    \n",
    "        #generate list of indexes\n",
    "        gen = (x for x in range(j+1,j+4) if x<len(words_in_row))\n",
    "        for k in gen:\n",
    "            #avoid self-loops\n",
    "            if(word == words_in_row[k]):\n",
    "                pass\n",
    "            #if edge does not exist create it, else increase edge weight\n",
    "            elif((word, words_in_row[k]) not in g.edges):\n",
    "                g.add_edge(word, words_in_row[k])\n",
    "                g[word][words_in_row[k]]['weight'] = 1\n",
    "            else:\n",
    "                g[word][words_in_row[k]]['weight'] += 1\n",
    "    return g\n",
    "def generate_graphs(total_documents:int, chunksize:int, database_collection):\n",
    "    chunk = []\n",
    "    counter = 0\n",
    "    ticks = [x for x in range(0,total_documents,chunksize)]\n",
    "    ticks.append(total_documents)\n",
    "    graphs=[]\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    for j in range(len(ticks)-1):\n",
    "        tokens = {}\n",
    "        #read documents from MongoDB in chunks\n",
    "        chunk = list(database_collection.find({ }, { \"_id\": 1, \"speech\": 1 })[ticks[j]:ticks[j+1]])\n",
    "        print(\"Length of chunk: \", len(chunk))\n",
    "        size_distribution = []\n",
    "        #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "        #for each speech\n",
    "        for i, row in enumerate(chunk):\n",
    "            words_in_row = ind.preprocess_doc(row[\"speech\"], stopwords)\n",
    "            graph = create_graph_of_words(words_in_row)\n",
    "            graphs.append(graph)\n",
    "            #extract keywords and insert to mongo\n",
    "   \n",
    "        \n",
    "        print(\"CHUNK\", counter, \" FINISHED\")\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def weighted_undirected_k_core(graph:nx.Graph):\n",
    "    k = 0\n",
    "    core_numbers = dict([(node,0) for node in graph.nodes])\n",
    "    degrees = dict(graph.degree(weight='weight'))\n",
    "    while(len(graph.nodes)>0):\n",
    "        min_node = get_min_node(degrees)\n",
    "        #print(min_node)\n",
    "        if(degrees[min_node]>k):\n",
    "            core_numbers[min_node] = degrees[min_node]\n",
    "            k = core_numbers[min_node]\n",
    "        else:\n",
    "            core_numbers[min_node] = k\n",
    "        for neighbor in graph.neighbors(min_node):\n",
    "            degrees[neighbor] = degrees[neighbor]  - graph.get_edge_data(neighbor, min_node)['weight']\n",
    "        graph.remove_node(min_node)\n",
    "        del degrees[min_node]\n",
    "        \n",
    "    return core_numbers\n",
    "\n",
    "def get_min_node(degrees):\n",
    "    #sort degrees in increasing orders\n",
    "    degrees = sorted(degrees.items(), key=lambda d: d[1])\n",
    "    return degrees[0][0]\n",
    "\n",
    "#returns the keywords for a document\n",
    "def get_keywords(database_collection, document_id:string)->list:\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    tokens = {}\n",
    "    #read documents from MongoDB in chunks\n",
    "    document = list(database_collection.find({\"_id\":document_id}, { \"_id\": 0, \"speech\": 1 }))\n",
    "    size_distribution = []\n",
    "    #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "    #for each speech\n",
    "    words_in_row = ind.preprocess_doc(document[0]['speech'], stopwords)\n",
    "    graph = create_graph_of_words(words_in_row)\n",
    "    #extract keywords and insert to mongo\n",
    "    #core_numbers = weighted_undirected_k_core(graph)\n",
    "    centrality = nx.betweenness_centrality(graph)\n",
    "    centrality = dict(sorted(centrality.items(), key=lambda item: -item[1]))\n",
    "    print(centrality)\n",
    "    words = list(centrality.keys())\n",
    "    keywords = words[:int(len(words_in_row)/4)]\n",
    "\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "#mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'πολιτ': 0.3572644608551125, 'κυβερνης': 0.1670047360514471, 'ειπ': 0.12939892316319912, 'πραγματ': 0.1104672301027447, 'συνασπισμ': 0.10569015486580276, 'νε': 0.09694859568419974, 'δημοκρατ': 0.09586480901636865, 'συγκυβερνης': 0.08738160619723236, 'πασοκ': 0.08246529959489976, 'κρατ': 0.06846865595423295, 'αποφας': 0.06726673326938995, 'μιλ': 0.06438328745227527, 'μινιμουμ': 0.05756231194120392, 'θεμ': 0.05656285993770532, 'παρ': 0.05173578290186787, 'κυβερν': 0.051023707198146466, 'επρεπ': 0.044266165660291734, 'χρον': 0.04302205549830263, 'πανω': 0.0409863777387555, 'ομοψυχ': 0.03803534002301355, 'γνωριζ': 0.03684056018339678, 'διευθυντ': 0.035545237831017935, 'μερ': 0.031476789488799194, 'ακυβερνης': 0.02875234951998579, 'τρεχ': 0.026568206372375475, 'θετ': 0.023719695704989826, 'εσωτερ': 0.023184711850695105, 'στιγμ': 0.022785349833857504, 'εμπιστοσυν': 0.022015630402358898, 'υπαρχ': 0.021533636366399225, 'αρχηγ': 0.018797862644233308, 'μητσοτακ': 0.01792481038767405, 'καταδειχθ': 0.01756879199038839, 'διαστημ': 0.015424168177024506, 'αντιφατ': 0.014779218270542242, 'υποχρεωμεν': 0.013539168962193237, 'τρεχοντ': 0.013531586621687602, 'νομαρχ': 0.013364644533067446, 'εκλογ': 0.013071436689731509, '18': 0.012846129786004187, 'παρασκην': 0.012535065506599894, 'προωθ': 0.0115190739909901, 'κρισιμ': 0.011006781386728295, 'συναδελφ': 0.01091789570628341, 'παραγοντ': 0.010393509824426375, 'αληθ': 0.01011592090676688, 'εθν': 0.010031335943605564, 'λεξ': 0.009721958200728784, 'δημος': 0.009550034654016727, 'δυο': 0.009399054739618872, 'φευγ': 0.009311793445477447, 'ελεγ': 0.009244175678740972, 'επραξ': 0.00921720553911578, 'ερωτ': 0.008850576660893247, \"σ'\": 0.008778451133727548, 'προσκην': 0.008747284690395766, 'αφορ': 0.008201364169478862, 'κρις': 0.008000285687435233, 'αναφερθ': 0.007788910424716966, 'τεκταιν': 0.007634986197113178, 'υπουργ': 0.0075620346070163465, 'ξερ': 0.007510260166016638, 'ζω': 0.00735102169782415, 'υπαρξ': 0.0070744923988845035, 'δεξ': 0.006649403862111433, 'προβλημ': 0.006621040290459819, 'ηθελ': 0.006597418636897144, 'παιρν': 0.006435532576877765, 'μαρτ': 0.005625069081350227, 'λ': 0.005591942955096819, 'μην': 0.005544548575880359, 'προθυρ': 0.005397409923608373, 'κριν': 0.0051757431578623074, 'αναλαμβαν': 0.005081987500773059, 'ταυτοχρον': 0.005079013714467184, 'ευσταθ': 0.005006635563994871, 'κομματικοποι': 0.004847537925652254, 'βρισκ': 0.004826675302023753, 'καταλαβ': 0.0046547409184390185, 'διλημμ': 0.004343933673273548, 'προγραμμ': 0.004321988078684985, 'αποκομματικοποιης': 0.0041752404893657235, 'καθημεριν': 0.0037856670872311146, 'καλ': 0.0037560530225806336, 'φλωρακ': 0.003333725824372077, 'τροπ': 0.0033291428808497397, 'καν': 0.0032010170332702013, 'απλως': 0.0030400831708376082, 'τοπ': 0.0030215355215355223, 'σημαιν': 0.0029805854805854816, 'γιατι': 0.0028276430317344327, 'συμπολιτευς': 0.0027097754504171612, 'λεγομεν': 0.002599785153261089, 'σ': 0.0024569998403733235, 'επειδη': 0.002415496947113492, 'συντελεστ': 0.0022755394852148987, 'παρθ': 0.002090779144398188, 'αντιμετωπιζ': 0.0017940510810470762, 'γιν': 0.001561929793105842, 'γνωστ': 0.001557206235157507, 'θες': 0.001474386651826901, 'κομματικοποιημεν': 0.0014260727358813485, 'αμες': 0.0013052209544820016, 'τιμι': 0.0012563468870864266, '1987': 0.0012450752586461542, 'εθες': 0.0009320548742972507, 'δεχ': 0.000436275957666332, 'τοποθετης': 0.00042142393328164086, 'κομμ': 0.00019792519792519793, 'αμφισβητ': 0.0, 'δυνατον': 0.0, 'συνασπιζ': 0.0}\n",
      "['πολιτ', 'κυβερνης', 'ειπ', 'πραγματ', 'συνασπισμ', 'νε', 'δημοκρατ', 'συγκυβερνης', 'πασοκ', 'κρατ', 'αποφας', 'μιλ', 'μινιμουμ', 'θεμ', 'παρ', 'κυβερν', 'επρεπ', 'χρον', 'πανω', 'ομοψυχ', 'γνωριζ', 'διευθυντ', 'μερ', 'ακυβερνης', 'τρεχ', 'θετ', 'εσωτερ', 'στιγμ', 'εμπιστοσυν', 'υπαρχ', 'αρχηγ', 'μητσοτακ', 'καταδειχθ', 'διαστημ', 'αντιφατ', 'υποχρεωμεν', 'τρεχοντ', 'νομαρχ', 'εκλογ', '18', 'παρασκην', 'προωθ', 'κρισιμ', 'συναδελφ', 'παραγοντ', 'αληθ', 'εθν']\n"
     ]
    }
   ],
   "source": [
    "#get keywords for a single document\n",
    "keywords = get_keywords(database, \"234\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, pls ignore\n",
    "dataframe1 = list(collection.find({ }, { \"_id\": 1, \"speech\": 1 })[:100000])\n",
    "for i, row in enumerate(dataframe1):\n",
    "    #print(row[\"_id\"])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(graphs[0].edges)\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.savefig('graph.png')\n",
    "# larger figure size\n",
    "#pos = nx.circular_layout(graphs[2])\n",
    "\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw_networkx(graphs[0], with_labels=True,node_size=80,font_size=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networkx library test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx library\n",
    "import networkx as nx\n",
    "g = nx.Graph()\n",
    "l = ['1','2','3']\n",
    "#g.add_nodes_from(l)\n",
    "\n",
    "for word in l:\n",
    "    if(word not in list(g.nodes)):\n",
    "        g.add_node(word)\n",
    "for e in list(g.nodes):\n",
    "    print(e)\n",
    "    \n",
    "g.add_edge('1','2')\n",
    "g.add_edge('2','3')\n",
    "if(('1','2') in g.edges):\n",
    "    print(' in')\n",
    "for e in list(g.edges):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphs = generate_graphs(100,100,database)\n",
    "graph = graphs[78]\n",
    "for node in graph.nodes:\n",
    "    for neighbor in graph.neighbors(node):\n",
    "        if(graph.get_edge_data(neighbor, node)['weight']>1):\n",
    "            print(graph.get_edge_data(neighbor, node)['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_numbers = dict(core_numbers)\n",
    "max_core = [num for num in core_numbers.values() if(num==2)]\n",
    "print(len(max_core))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nx.Graph()\n",
    "test.add_nodes_from([1,2,3,4,5,6])\n",
    "test.add_edge(1,2)\n",
    "test[1][2]['weight'] = 2\n",
    "test.add_edge(1,4)\n",
    "test[1][4]['weight'] = 4\n",
    "\n",
    "test.add_edge(2,3)\n",
    "test[2][3]['weight'] = 3\n",
    "\n",
    "test.add_edge(3,4)\n",
    "test[3][4]['weight'] = 5\n",
    "\n",
    "test.add_edge(3,5)\n",
    "test[3][5]['weight'] = 7\n",
    "\n",
    "test.add_edge(4,5)\n",
    "test[4][5]['weight'] = 6\n",
    "\n",
    "test.add_edge(5,6)\n",
    "test[5][6]['weight'] = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "core_numbers = dict([(node,0) for node in test.nodes])\n",
    "#print(graph.nodes)\n",
    "print('sdfdsf.',len(nx.k_core(test).nodes))\n",
    "degrees = dict(test.degree(weight='weight'))\n",
    "while(len(test.nodes)>0):\n",
    "    min_node = get_min_node(degrees)\n",
    "    #print(min_node)\n",
    "    if(degrees[min_node]>k):\n",
    "        core_numbers[min_node] = degrees[min_node]\n",
    "        k = core_numbers[min_node]\n",
    "    else:\n",
    "        core_numbers[min_node] = k\n",
    "    for neighbor in test.neighbors(min_node):\n",
    "        print(neighbor)\n",
    "        degrees[neighbor] = degrees[neighbor]  - test.get_edge_data(min_node, neighbor)['weight']\n",
    "    test.remove_node(min_node)\n",
    "    del degrees[min_node]\n",
    "    \n",
    "print(sorted(core_numbers.items(), key=lambda d: d[1]))\n",
    "print(len(core_numbers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
