{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string \n",
    "import unicodedata as ud\n",
    "from greek_stemmer import GreekStemmer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import indexer as ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_of_words(words_in_row:list)->nx.DiGraph:\n",
    "    g = nx.DiGraph()\n",
    "    #add unique words as nodes of the graph\n",
    "    g.add_nodes_from(set(words_in_row))\n",
    "    for j,word in enumerate(words_in_row):\n",
    "        #add word as a node if it doesnt exist\n",
    "        #if word not in g.nodes:\n",
    "            #g.add_node(word)    \n",
    "        #generate list of indexes\n",
    "        gen = (x for x in range(j+1,j+4) if x<len(words_in_row))\n",
    "        for k in gen:\n",
    "            #avoid self-loops\n",
    "            if(word == words_in_row[k]):\n",
    "                pass\n",
    "            #if edge does not exist create it, else increase edge weight\n",
    "            elif((word, words_in_row[k]) not in g.edges):\n",
    "                g.add_edge(word, words_in_row[k])\n",
    "                g[word][words_in_row[k]]['weight'] = 1\n",
    "            else:\n",
    "                g[word][words_in_row[k]]['weight'] += 1\n",
    "    return g\n",
    "def generate_graphs(total_documents:int, chunksize:int, database_collection):\n",
    "    chunk = []\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    #get inverted index\n",
    "    ticks = [x for x in range(0,total_documents,chunksize)]\n",
    "    ticks.append(total_documents)\n",
    "    graphs=[]\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    for j in range(len(ticks)-1):\n",
    "        tokens = {}\n",
    "        chunk = list(database_collection.find({ }, { \"_id\": 1, \"speech\": 1 })[ticks[j]:ticks[j+1]])\n",
    "        print(\"Length of chunk: \", len(chunk))\n",
    "        size_distribution = []\n",
    "        #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "        #for each speech\n",
    "        for i, row in enumerate(chunk):\n",
    "            words_in_row = ind.preprocess_doc(row[\"speech\"], stopwords)\n",
    "            graph = create_graph_of_words(words_in_row)\n",
    "            graphs.append(graph)\n",
    "   \n",
    "        #insert chunk of tokens to a mongo collection named index\n",
    "        \n",
    "        print(\"CHUNK\", counter, \" FINISHED\")\n",
    "    \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    #mongo_client.drop_database(\"GreekParliamentProceedings\")\n",
    "client = mongo_client[\"GreekParliamentProceedings\"]\n",
    "index = client[\"InvertedIndex\"]\n",
    "database = client[\"Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunk:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\greek_stemmer\\__init__.py:340: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  custom_rules = yaml.load(f.read())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK 0  FINISHED\n"
     ]
    }
   ],
   "source": [
    "graphs = generate_graphs(1000,1000,database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphs[0].edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, pls ignore\n",
    "dataframe1 = list(collection.find({ }, { \"_id\": 1, \"speech\": 1 })[:100000])\n",
    "for i, row in enumerate(dataframe1):\n",
    "    #print(row[\"_id\"])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(graphs[0].edges)\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.savefig('graph.png')\n",
    "# larger figure size\n",
    "#pos = nx.circular_layout(graphs[2])\n",
    "\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw_networkx(graphs[0], with_labels=True,node_size=80,font_size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(graphs)):\n",
    "    if(len(graphs[i].nodes)>400):\n",
    "        res = nx.k_core(graphs[i])\n",
    "        print(len(res.nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networkx library test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx library\n",
    "import networkx as nx\n",
    "g = nx.Graph()\n",
    "l = ['1','2','3']\n",
    "#g.add_nodes_from(l)\n",
    "\n",
    "for word in l:\n",
    "    if(word not in list(g.nodes)):\n",
    "        g.add_node(word)\n",
    "for e in list(g.nodes):\n",
    "    print(e)\n",
    "    \n",
    "g.add_edge('1','2')\n",
    "g.add_edge('2','3')\n",
    "if(('1','2') in g.edges):\n",
    "    print(' in')\n",
    "for e in list(g.edges):\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
