{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string \n",
    "import unicodedata as ud\n",
    "from greek_stemmer import GreekStemmer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import re\n",
    "import networkit as nk\n",
    "\n",
    "\n",
    "def tokenize(row):\n",
    "    return word_tokenize(row)\n",
    "\n",
    "def preprocess_doc(doc: str) -> list:\n",
    "    stemmer = GreekStemmer()\n",
    "    d = {ord('\\N{COMBINING ACUTE ACCENT}'):None}\n",
    "    # load stopwords\n",
    "    with open('stopwords.txt', encoding='utf-8') as file:\n",
    "        stopwords = [line.rstrip() for line in file]\n",
    "    #doc = doc.lower()\n",
    "    if doc!=\"\":\n",
    "        words = [stemmer.stem(ud.normalize('NFD',w).upper().translate(d)).lower() for w in filter(None, re.split(\"[,~`; _'.\\-!?:]+\",doc)) if w not in stopwords and w not in string.punctuation]\n",
    "    return words\n",
    "\n",
    "def create_index(dataframe):\n",
    "    chunk = []\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in dataframe:\n",
    "        tokens = {}\n",
    "        chunk = (data[\"speech\"].values.tolist())\n",
    "        print(\"Length of chunk: \", len(chunk))\n",
    "        #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "        for i, row in enumerate(chunk):\n",
    "            words_in_row = preprocess_doc(row.lower())\n",
    "            '''\n",
    "            \n",
    "            TOKENIZED ROWS IN ROW\n",
    "            We now need to remove stopwords, perform stemming and remove punctuation\n",
    "            Afterwards, we can add them to the index. This will be done for each row of the dataframe chunk.\n",
    "            When the chunk is finished, we will create a json file with the index of this chunk and we'll do the process again for the next chunk.\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            for word in words_in_row:\n",
    "                \n",
    "                if word in tokens.keys():\n",
    "                    #term already in index\n",
    "                    if i not in tokens[word][\"postinglist\"].keys():\n",
    "                        #term in other document\n",
    "                        #n = tokens[word][\"numdoc\"]\n",
    "                        tokens[word][\"postinglist\"][str(i)] = words_in_row.count(word)\n",
    "                        tokens[word][\"numdoc\"] = len(tokens[word][\"postinglist\"])\n",
    "\n",
    "                    else:\n",
    "                        #term in same document\n",
    "                        n = tokens[word][\"numdoc\"]\n",
    "                        tokens[word][\"numdoc\"] = n+1\n",
    "                        pass\n",
    "                        \n",
    "                else:\n",
    "                    #term not in index\n",
    "                    tokens[word]={\"numdoc\":1, \"postinglist\":{str(i) : words_in_row.count(word)}}\n",
    "                \n",
    "                \n",
    "        \n",
    "        counter +=1\n",
    "        print(\"CHUNK\", counter, \" FINISHED\")\n",
    "        print(\"Number of Tokens: \", len(tokens))\n",
    "        \n",
    "        print(type(tokens))\n",
    "        if(counter == 1):\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "def has_node(graph, name):\n",
    "    try:\n",
    "        graph.vs.find(name=name)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def create_index(dataframe):\n",
    "    chunk = []\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in dataframe:\n",
    "        tokens = {}\n",
    "        chunk = (data[\"speech\"].values.tolist())\n",
    "        print(\"Length of chunk: \", len(chunk))\n",
    "        #chunk = [\"This is a sentance\",\"This is another one\"]\n",
    "        #1 row is a document\n",
    "        graphs = {}\n",
    "        for i, row in enumerate(chunk):\n",
    "            words_in_row = preprocess_doc(row.lower())\n",
    "            graph = nk.Graph(len(words_in_row), weighted=True, directed=True)\n",
    "            att=graph.attachNodeAttribute('name',str)\n",
    "            print('WORDS IN DOC: ', (words_in_row)) \n",
    "            for j,word in enumerate(words_in_row):\n",
    "                att[j]=word\n",
    "                print(graph.name(j))\n",
    "                k=1\n",
    "                while(k<4 and k+j<len(words_in_row)):\n",
    "                    if(graph.hasEdge(j, k+j)==False):\n",
    "                        graph.addEdge(j, k+j)\n",
    "                    else:\n",
    "                        new_weight = graph.weight(j,k+j) + 1\n",
    "                        print('updating weight')\n",
    "                        graph.increaseWeight(j, j+k, new_weight)\n",
    "                    k+=1\n",
    "                    \n",
    "            graphs[i]=graph\n",
    "            print(row)\n",
    "            break\n",
    "                \n",
    "                \n",
    "        \n",
    "        counter +=1\n",
    "        print(\"CHUNK\", counter, \" FINISHED\")\n",
    "        print(\"Number of Tokens: \", len(tokens))\n",
    "        \n",
    "        print(type(tokens))\n",
    "        if(counter == 1):\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return tokens, graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunk:  1000\n",
      "WORDS IN DOC:  ['παρακαλειτα', 'γραμματ', 'βουλγαρακ', 'συνοδευς', 'μακαρι', 'αρχιεπισκοπ', 'αθην', 'πας', 'ελλαδ', 'σεραφειμ', 'συνοδευοντ', 'αυτον', 'μελ', 'ιερ', 'συνοδ', 'κατα', 'εισοδ', 'αιθους', 'βουλ', 'προκειμεν', 'τελεσθ', 'αγιασμ', 'συνεχει', 'τελειτα', 'καθιερωμεν', 'αγιασμ']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'networkit.graph.Graph' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [122], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mGreek_Parliament_Proceedings_1989_2020.csv\u001b[39m\u001b[39m'\u001b[39m, chunksize\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m index, graphs \u001b[39m=\u001b[39m create_index(dataframe)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(graphs[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn [121], line 28\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m j,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words_in_row):\n\u001b[0;32m     27\u001b[0m     att[j]\u001b[39m=\u001b[39mword\n\u001b[1;32m---> 28\u001b[0m     \u001b[39mprint\u001b[39m(graph\u001b[39m.\u001b[39;49mname(j))\n\u001b[0;32m     29\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     30\u001b[0m     \u001b[39mwhile\u001b[39;00m(k\u001b[39m<\u001b[39m\u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m k\u001b[39m+\u001b[39mj\u001b[39m<\u001b[39m\u001b[39mlen\u001b[39m(words_in_row)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'networkit.graph.Graph' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('Greek_Parliament_Proceedings_1989_2020.csv', chunksize=1000)\n",
    "index, graphs = create_index(dataframe)\n",
    "print(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=graphs[0]\n",
    "g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graphs[0]\n",
    "graph.write_svg('/Users/harry/a.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkit.graph.NodeAttribute at 0x2677077f460>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkit as nk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
